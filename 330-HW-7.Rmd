---
title: "HW 7 Diabetes"
author: "Chris Trippe & Ethan Kemeny"
date: "11/27/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(message = FALSE)
```
```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(MASS)
library(lmtest)
library(bestglm)
library(GGally)
library(car)
library(ggcorrplot)
library(kableExtra)
library(gridExtra)
library(pROC)
```

```{r, message = FALSE, echo = FALSE}
#data clean-up
diabetes_data <- read_table2("diabetes.txt")

##Remove all rows where BMI = 0
diabetes_bmi <- as.data.frame(filter(diabetes_data, bmi != 0))

##Remove all rows where Insulin = 0
diabetes_age <- as.data.frame(filter(diabetes_bmi, insulin != 0))

##age 81 is an outlier. Delete and line for age is monotonic
diabetes_clean <- diabetes_age[-which(diabetes_age$age == 81),]

```


1. 

Type 2 Diabetes is connected to various health problems.  Being able to detect diabetes early can help manage and prevent those health problems.  Doctors would like to be able to understand how various physical factors such as bmi and age impact the likelihood a person has diabetes.  This would allow them to better identify patients who truly have diabetes and help them alleviate and/or prevent health complications associated with type 2 diabetes.

2. 

```{r, echo = FALSE, message = FALSE}
insulin_clean_plot <- ggplot(diabetes_clean,aes(insulin,diabetes)) + geom_jitter(height = 0.1) + geom_smooth(se=FALSE) + ggtitle("Clean Data: Insulin")

insulin_raw_plot <- ggplot(diabetes_data,aes(insulin,diabetes)) + geom_jitter(height = 0.1) + geom_smooth(se=FALSE) + ggtitle("Raw Data: Insulin")

age_clean_plot <- ggplot(diabetes_clean,aes(age,diabetes)) + geom_jitter(height = 0.1) + geom_smooth(se=FALSE) + ggtitle("Clean Data: Age")

age_raw_plot <- ggplot(diabetes_data,aes(age,diabetes)) + geom_jitter(height = 0.1) + geom_smooth(se=FALSE) + ggtitle("Raw Data: Age")

grid.arrange(insulin_raw_plot,insulin_clean_plot,nrow=1)
grid.arrange(age_raw_plot,age_clean_plot,nrow=1)
```

The data we received has some errors in it we needed to fix.  All patients who had a recorded bmi of 0 or insulin levels recorded at 0 were removed.  We also removed the patient with an age of 81 because it was a severe outlier that heavily affected monotonality. While not perfectly monotonic, with the removal of the age 81 patient, the data is mostly monotonic and is enough so to justify doing logistic regression.

Multiple linear regression is not a valid method for this data because the response variable is in terms of 'yes' or 'no' rather than numeric values. When reading yes's and no's as 1's and 0's respectively, our predictions won't always be 1's and zero's. Many of the assumptions such as linearity, normality, and equal variance are likely to be violted. As such we logistic regression fits better because it is for dealing with 'yes' or 'no' response variables and the probability associated with predicting this type of response.



3. 

```{r, echo = FALSE, fig.height = 3, fig.width = 3}
vs.res <- bestglm(diabetes_clean, IC = "BIC", method = "exhaustive", family = binomial)

best_slr <- vs.res$BestModel
# summary(best_slr)

plot(vs.res$Subsets$BIC,type="b",pch=19,xlab="# of Vars",
ylab="BIC") 
```

We decided to use BIC because we wanted more focus on interpreting the factors that influence the likelihood a person has diabetes.  The algorithm we chose was exhaustive because there aren't that many factors in the data so we can test every possible model to determine the best one.  Using this method we found the most important variables to be glucose, bmi, pedigree, and age.

4.

$$log(\frac{p_i}{1+p_i}) = \beta_0 + \beta_1(glucose_i) + \beta_2(bmi_i) + \beta_3(pedigree_i) +\beta_4(age_i)$$

For this model to work, we assume linear probability and independence.  Becuase the data was mostly montonic, we can assume linear probability.  We can also assume independence because one person having diabetes should not influence the chance that another random person has diabetes unless they're related. We will assume in this case that most of the people whose data we used aren't related.

5. 

```{r, collapse=TRUE , }
log.odds <- confint(best_slr) %>% as.data.frame(.) ##log-odds
times <- exp(confint(best_slr)) %>% as.data.frame(.) ##times
percentage <- 100*(exp(confint(best_slr))-1) %>% as.data.frame(.) ##percentage


# 
# log.odds <-(as.data.frame(confint(best_slr)))
names(log.odds) <- c("Lower Bound", "Upper Bound")
names(times) <- c("Lower Bound", "Upper Bound")
names(percentage) <- c("Lower Bound", "Upper Bound")

```
<center> __Log Odds 95% Confidence Interval__ 
```{r}
knitr::kable(log.odds, align = "c", format = "markdown" ,digits = 5)
```
<center> __Times Likely 95% Confidence Interval__
```{r}
knitr::kable(times, align = "c",format = "markdown",digits = 5 )
```
<center> __Percentage 95% Confidence Interval__
```{r}
knitr::kable(percentage, align = "c",format = "markdown",digits = 5)
```

Looking at the table for Percentage 95% Confidence Interval, we can see that the interval for age is between `r percentage[5,1]` and `r percentage[5,2]`. This means that as age goes up by 1, holding all else constant, a person's chance of having diabetes goes up between `r percentage[5,1]`% and `r percentage[5,2]`%

6. 

```{r, echo = FALSE}
pred.probs <- predict.glm(best_slr, type = "response")

thresh <- seq(0,1, length = 100)

misclass <- rep (NA, length = length(thresh))

for(i in 1:length(thresh)) {
#If probability greater than threshold then 1 else 0
my.classification <- ifelse(pred.probs>thresh[i],1,0)
# calculate the pct where my classification not eq truth
misclass[i] <- mean(my.classification!=diabetes_clean$diabetes)
}

#Find threshold which minimizes misclassification
cutoff <- thresh[which.min(misclass)]


plot(thresh, misclass, pch = 19, main = "Minimized Misclassification")
abline(v = cutoff, col = "red")
```

The appropriate threshold for classification that minimizes the misclassification is `r round(cutoff,4)`. When looking at the graph "Minimized Misclassification", 100 misclassification values between 0 and 1 were calculated. The vertical red line represents the minimum threshold.

7. 

```{r, echo = FALSE}
## Use fitted model to predict
pred.probs <- predict.glm(best_slr, type="response") #response gives probabilities

## Classify according to threshold
test.class <- ifelse(pred.probs>cutoff,1,0)

## Create a confusion matrix
conf.mat <- addmargins(table(factor(diabetes_clean$diabetes,levels=c(0,1)),factor(test.class,levels=c(0,1))))
rownames(conf.mat) <- c("True No", "True Yes", "Sum")
colnames(conf.mat) <- c("Pred No", "Pred Yes", "Sum")
conf.mat

SENS <- conf.mat[2,2]/conf.mat[2,3]
SPEC <- conf.mat[1,1]/conf.mat[1,3]
PPV <- conf.mat[2,2]/conf.mat[3,2]
NPV <- conf.mat[1,1]/conf.mat[3,1]

SENS
SPEC
PPV
NPV

##pseudo R^2
r2 <- 1-best_slr$deviance/best_slr$null.deviance
r2

##AUC
a.roc <- roc(diabetes_clean$diabetes,pred.probs)
AUC <- auc(a.roc)

plot(a.roc,legacy.axes=TRUE)
abline(h = 0)
abline(v = 0)
```

According to our pseudo, $R^2 $ value of `r round(r2,4)`, `r 100*round(r2,4)` percent of the variance in the log-odds of diabetes is explained away by the explanatory variables used in our model.

As for the area under our Receiver Characteristic Operator curve, we obtain a value of `r AUC`. Normally, if we had to guess if someone had diabetes, it would be a 50/50 chance--either they have diabetes or they don't. An AUC (area under the curve) value of `r AUC` means that any given person has a `r 100*AUC` percent chance of not having diabetes and a `r 100*(1-AUC)` percent chance of having diabetes, on average. 

From the data, we obtain a sensitivity value of `r round(mean(SENS),4)`, a specitivity value of `r round(mean(SPEC), 4)`, a positive predictive value of `r round(mean(PPV),4)`, and a negative predictive value of `r round(mean(NPV),4)`. This means we correctly identified `r 100*round(mean(SENS),4)` percent of people that actually had diabetes and `r 100*round(mean(SPEC),4)` percent of people that did not have diabetes. Of those that we predicted to have diabetes, `r 100*round(mean(PPV),4)` actually had diabetes and of those that we predicted to not have diabetes, `r 100*round(mean(NPV),4)` percent of them did not actually have diabetes. These are all fairly high percentages which would allow us to conclude that this model fits the data well. The one concern is the sensitivity which would be better it if were a little higher. It's better to overdiagnose people with diabetes so that the likelihood of those that do have diabetes test positive. 

8. 

```{r, echo = FALSE}
n.cv <- 500
n.test <- round(.1*nrow(diabetes_clean))

sens <- rep(NA,n.cv)
spec <- rep(NA,n.cv)
ppv <- rep(NA,n.cv)
npv <- rep(NA,n.cv)
auc <- rep(NA,n.cv)

## Begin for loop
for(cv in 1:n.cv){
## Separate into test and training sets
test.obs <- sample(nrow(diabetes_clean),n.test)
test.set <- diabetes_clean[test.obs,]
train.set <- diabetes_clean[-test.obs,]

## Fit best model to training set
train.model <- glm(diabetes~glucose+pedigree+age+bmi,data=train.set,family=binomial)

## Use fitted model to predict test set
pred.probs <- predict.glm(train.model, newdata=test.set, type="response") #response gives probabilities

## Classify according to threshold
test.class <- ifelse(pred.probs>cutoff,1,0)

## Create a confusion matrix
conf.mat <- addmargins(table(factor(test.set$diabetes,levels=c(0,1)),factor(test.class,levels=c(0,1))))

## Pull of sensitivity, specificity, PPV and NPV
## using bracket notation
sens[cv] <- conf.mat[2,2]/conf.mat[2,3]
spec[cv] <- conf.mat[1,1]/conf.mat[1,3]
ppv[cv] <- conf.mat[2,2]/conf.mat[3,2]
npv[cv] <- conf.mat[1,1]/conf.mat[3,1]
## Calculate AUC
auc[cv] <- auc(roc(test.set$diabetes,pred.probs))
} #End for-loop

mean(sens)
mean(spec)
mean(ppv)
mean(npv)
```

When running a cross-validation on the data, we obtain a sensitivity value of `r round(mean(sens),4)`, a specitivity value of `r round(mean(spec), 4)`, a positive predictive value of `r round(mean(ppv),4)`, and a negative predictive value of `r round(mean(npv),4)`. This means we correctly identified `r 100*round(mean(sens),4)` percent of people that actually had diabetes and `r 100*round(mean(spec),4)` percent of people that did not have diabetes. Of those that we predicted to have diabetes, `r 100*round(mean(ppv),4)` actually had diabetes and of those that we predicted to not have diabetes, `r 100*round(mean(npv),4)` percent of them did not actually have diabetes. These are all fairly high percentages which would allow us to conclude that this model is fairly good at predicting. Once again, the only slightly worrisome percentage is the sensitivity which would be better if it were a little higher. 

9.

```{r, echo = FALSE}
dframe <- data.frame( glucose= 90, bmi= 25.1, pedigree= 1.268, age= 25)
pred.val <- predict.glm(best_slr,dframe, interval = "prediction", type = "response")

```

When using our model to predict whether or not a person with glucose = 90, bmi = 25.1, pedigree = 1.268, and age = 25, we obtain a value of `r pred.val`. Our calculated cutoff value is `r cutoff` which is greater than `r pred.val`. Therefore, we conclude that a person with these particular traits would not have diabetes. 